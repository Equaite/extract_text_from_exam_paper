{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import base64\n",
    "import concurrent.futures\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from dotenv import load_dotenv\n",
    "from itertools import compress\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from typing import List\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def extract_json_from_markdown(markdown_string):\n",
    "    # Remove the triple backticks and 'json' from the input string\n",
    "    json_string = markdown_string.strip().strip('```json').strip('```').strip()\n",
    "    \n",
    "    # Parse the cleaned JSON string into a Python dictionary\n",
    "    data = json.loads(json_string)\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    else: \n",
    "        return [data]\n",
    "\n",
    "def extract_tuple(text):\n",
    "    # Define the regex patterns to find the tuples\n",
    "    pattern_ab = r\"\\('([a-z])', '([a-z])'\\)\"\n",
    "    pattern_c = r\"\\('([a-z])',?\\)\"\n",
    "    pattern_empty = r\"\\(\\)\"\n",
    "    \n",
    "    # Search for the tuples in the text\n",
    "    match_ab = re.search(pattern_ab, text)\n",
    "    match_c = re.search(pattern_c, text)\n",
    "    match_empty = re.search(pattern_empty, text)\n",
    "    \n",
    "    # Check for the tuples and return the corresponding result\n",
    "    if match_ab:\n",
    "        return match_ab.groups()\n",
    "    elif match_c and not match_ab:\n",
    "        return (match_c.groups()[0],)\n",
    "    elif match_empty:\n",
    "        return ()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def merge_dict_on_key(dicts: List, key: str):\n",
    "\n",
    "    if len(dicts) == 1:\n",
    "        comb_dict = dicts[0]\n",
    "    else:\n",
    "        comb_dict = {}\n",
    "\n",
    "        values_from_dict = [dict[key] for dict in dicts]\n",
    "        joined_values_from_dict = \" \".join(values_from_dict)\n",
    "\n",
    "        comb_dict = {**dicts[0]}\n",
    "        comb_dict[key] = joined_values_from_dict\n",
    "        \n",
    "    return comb_dict\n",
    "\n",
    "def merge_dict_on_common_keys(dicts):\n",
    "\n",
    "  if len(dicts) == 1:\n",
    "      comb_dict = dicts[0]\n",
    "  else:\n",
    "      dict_keys = list(dicts[0].keys())\n",
    "      comb_dict = {}\n",
    "\n",
    "      for key in dict_keys:\n",
    "          vals = [record[key] for record in dicts] \n",
    "          dedup_vals = list(OrderedDict.fromkeys(vals))\n",
    "\n",
    "          if len(dedup_vals) == 1:\n",
    "              comb_dict[key] = dedup_vals[0]\n",
    "          else: \n",
    "              comb_dict[key] = \"\".join(dedup_vals)\n",
    "      \n",
    "  return comb_dict\n",
    "\n",
    "def dict_reorder(\n",
    "        input_dict, \n",
    "        keys_order\n",
    "        ):\n",
    "\n",
    "    ordered_dict = OrderedDict()\n",
    "\n",
    "    for _, key in enumerate(keys_order):\n",
    "        ordered_dict[key] = input_dict[key]\n",
    "\n",
    "    return ordered_dict\n",
    "\n",
    "def json_to_markdown(json_list):\n",
    "  \"\"\"\n",
    "  Converts a list of JSON objects to a markdown document.\n",
    "\n",
    "  Args:\n",
    "    json_list: A list of dictionaries representing JSON objects.\n",
    "\n",
    "  Returns:\n",
    "    A string containing the markdown document.\n",
    "  \"\"\"\n",
    "\n",
    "  markdown_text = \"\"\n",
    "  for item in json_list:\n",
    "    markdown_text += f\"{'-'*40}\\n\\n\"\n",
    "    for key, value in item.items():\n",
    "      # Convert value to string if it's not already\n",
    "      if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "      # Escape special characters for markdown\n",
    "      escaped_value = value.replace(\"#\", \"\\\\#\")\n",
    "      markdown_text += f\"## {key}\\n{escaped_value}\\n\\n\"\n",
    "  \n",
    "  return markdown_text\n",
    "\n",
    "def crop_image_borders_from_path(image_path, top_crop, bottom_crop, left_crop, right_crop):\n",
    "  \"\"\"Crops the borders of an image.\n",
    "\n",
    "  Args:\n",
    "    image_path: The path to the image file.\n",
    "    top_crop: Number of pixels to crop from the top.\n",
    "    bottom_crop: Number of pixels to crop from the bottom.\n",
    "    left_crop: Number of pixels to crop from the left.\n",
    "    right_crop: Number of pixels to crop from the right.\n",
    "\n",
    "  Returns:\n",
    "    The cropped image as a NumPy array.\n",
    "  \"\"\"\n",
    "\n",
    "  img = cv2.imread(image_path)\n",
    "  height, width, channels = img.shape\n",
    "\n",
    "  # Calculate new dimensions\n",
    "  new_height = height - top_crop - bottom_crop\n",
    "  new_width = width - left_crop - right_crop\n",
    "\n",
    "  # Crop the image\n",
    "  cropped_img = img[top_crop:height-bottom_crop, left_crop:width-right_crop]\n",
    "\n",
    "  return cropped_img\n",
    "\n",
    "def crop_image_borders(image, top_crop, bottom_crop, left_crop, right_crop):\n",
    "  \"\"\"Crops the borders of an image.\n",
    "\n",
    "  Args:\n",
    "    image: The PIL Image.\n",
    "    top_crop: Number of pixels to crop from the top.\n",
    "    bottom_crop: Number of pixels to crop from the bottom.\n",
    "    left_crop: Number of pixels to crop from the left.\n",
    "    right_crop: Number of pixels to crop from the right.\n",
    "\n",
    "  Returns:\n",
    "    The cropped image as a NumPy array.\n",
    "  \"\"\"\n",
    "\n",
    "  img = np.array(image)\n",
    "  height, width, channels = img.shape\n",
    "\n",
    "  # Calculate new dimensions\n",
    "  new_height = height - top_crop - bottom_crop\n",
    "  new_width = width - left_crop - right_crop\n",
    "\n",
    "  # Crop the image\n",
    "  cropped_img = img[top_crop:height-bottom_crop, left_crop:width-right_crop]\n",
    "\n",
    "  return cropped_img\n",
    "  \n",
    "def extract_parentheses_content(text):\n",
    "  \"\"\"Extracts the content within parentheses from a string.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "\n",
    "  Returns:\n",
    "    The extracted content within parentheses.\n",
    "  \"\"\"\n",
    "\n",
    "  match = re.search(r'\\((.*)\\)', text)\n",
    "  return match.group(0) if match else None\n",
    "\n",
    "def _merge_dicts(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "extract_paper_reference_prompt = \"\"\"\n",
    "You are given a cover page for an exam paper. \n",
    "Extract the following data from the page and express the output as a json object.\n",
    "\n",
    "1. Candidate Number\n",
    "2. Maximum mark for the paper\n",
    "3. The Subject of the Exam \n",
    "4. The Paper Reference\n",
    "5. The Exam Board that is issuing the paper\n",
    "6. The Paper number\n",
    "7. Month/Year of the Exam\n",
    "8. Title of the Exam paper\n",
    "\n",
    "Obey the following format\n",
    "\n",
    "{\n",
    "\"candidate_number\": int,\n",
    "\"max_mark\": int,\n",
    "\"exam_subject\": str,\n",
    "\"paper_reference\": str,\n",
    "\"exam_board\": str,\n",
    "\"paper_number\": int,\n",
    "\"month_year\": yyyy/mm,\n",
    "\"paper_title\": str\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "extract_question_parts_prompt = \"\"\"\n",
    "You are given a page from an exam paper. \n",
    "Extract the question parts mentioned in the page. \n",
    "Every question number will have a lower case letter denoting the question part, they will be enclosed in brackets.\n",
    "Read the page and return in the letters of the questions as a tuple.\n",
    "\n",
    "If no question parts are detected, return an empty tuple. Return nothing but the tuple.\n",
    "\"\"\"\n",
    "\n",
    "extract_question_parts_hist_prompt = \"\"\"\n",
    "You are given a page from an exam paper. \n",
    "Extract the question number mentioned in the page - the question number is enclosed in a box, with two squares, near the top left of the page.\n",
    "The question number is in printed text, it is not handwritten, do not return any handwritten numbers.\n",
    "Read the page and return question numbers identified in a tuple.\n",
    "\n",
    "If no question numbers are detected, return an empty tuple. Return nothing but the tuple.\n",
    "\"\"\"\n",
    "\n",
    "extract_handwriting_prompt = \"\"\"\n",
    "You are given a page from an exam paper. Your role is to extract the following details from the page:\n",
    "1. Handwritten Text\n",
    "\n",
    "Obey the following format\n",
    "\n",
    "{\n",
    "\"answer_text\": str\n",
    "}\n",
    "\n",
    "If there is no information about the fields requested, return the value of those fields to be an empty string. \n",
    "Do not invent anything.\n",
    "\"\"\"\n",
    "\n",
    "mcq_prompt = \"\"\"\n",
    "You are given a page from an exam paper. Your role is to extract the following details from the page:\n",
    "1. Printed Question Text\n",
    "2. Answer Text\n",
    "\n",
    "Obey the following format\n",
    "[{\n",
    "\"question_text\": str,\n",
    "\"answer_text\": str\n",
    "},\n",
    "{\n",
    "\"question_text\": str,\n",
    "\"answer_text\": str\n",
    "}]\n",
    "\n",
    "Here are some guidelines that you should follow:\n",
    "1. There are some multiple choice questions on this page, for these questions, extract only the question text and report only the letter for the selected options.\n",
    "2. If there are multiple questions, report all of them in a list of jsons obeying the format specified.\n",
    "3. If there is no information about the fields requested, return the value of those fields to be an empty string. \n",
    "4. Do not invent anything.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"edexcel_business_studies\"\n",
    "candidate_id = 5002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "pdfpath = Path(f\"./data/{subject_id}/\")\n",
    "\n",
    "images = convert_from_path(pdfpath / f\"{candidate_id}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to pages that have questions\n",
    "question_images = images[1:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a subset of pages\n",
    "image_savedir = Path(f\"./saved_imgs/{subject_id}/{candidate_id}\")\n",
    "image_savedir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define a function to save a single image\n",
    "def save_image(idx, image, image_savedir):\n",
    "    image.save(image_savedir / f\"image_page{idx}.png\", 'png')\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the saving process\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Create a list of tasks\n",
    "    tasks = [executor.submit(save_image, idx, image, image_savedir=image_savedir) for idx, image in enumerate(images)]\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    for task in tasks:\n",
    "        task.result()  # This will also re-raise any exceptions caught during the saving process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Paper Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_page_encoded = encode_image(image_savedir / \"image_page0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": model_name,\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{extract_paper_reference_prompt}\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{cover_page_encoded}\"\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"max_tokens\": 500\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "response_content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_reference = extract_json_from_markdown(response_content)[0][\"paper_reference\"]\n",
    "print(paper_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Images and Prepare DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = list(glob.glob(str(image_savedir)+\"/*.png\"))\n",
    "img_paths = sorted(img_paths)\n",
    "img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_num = [int( i.split(\".\")[0].split(\"/\")[-1].split(\"_\")[1][4:] ) for i in img_paths]\n",
    "encoded_imgs = [encode_image(v) for v in img_paths]\n",
    "\n",
    "print(dict(zip(img_paths, page_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataFrame\n",
    "img_df = pd.DataFrame({\"page_number\": page_num, \"paper_reference\": paper_reference, \"subject_id\": subject_id, \"encoded_image_cropped\": encoded_imgs})\n",
    "img_df = img_df.sort_values(by='page_number').reset_index(drop=True)\n",
    "img_df = img_df[1:22].reset_index(drop=True)\n",
    "img_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Question Number & Question Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to cache \n",
    "## Check if we have a saved dataframe corresponding to the paper reference\n",
    "dir_contents = glob.glob(\"./extracted_questions_by_page_number/**\", recursive=True)\n",
    "\n",
    "sanitised_paper_ref = paper_reference.replace(\"/\", \"-\")\n",
    "matching_files_lst = [i for i in dir_contents if sanitised_paper_ref in i]\n",
    "\n",
    "if len(matching_files_lst) == 0:\n",
    "    # Extract Question Numbers\n",
    "    # Add Question Parts - Use a multithreaded approach\n",
    "    def extract_question_part(idx, row):\n",
    "        print(\"Processing Page:\\n\", row[\"page_number\"])\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"{extract_question_parts_prompt}\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{row['encoded_image_cropped']}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 800\n",
    "        }\n",
    "\n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "        response_content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        return idx, response_content\n",
    "\n",
    "    extracted_tuples = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Map the process_row function to each row in the DataFrame, passing the index as well\n",
    "        futures = [executor.submit(extract_question_part, idx, row) for idx, row in img_df.iterrows()]\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                idx, response_content = future.result()\n",
    "                extracted_tuples.append((idx, response_content))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    # Sort the results by the original index to retain the order\n",
    "    extracted_tuples.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Extract just the responses in the correct order\n",
    "    extracted_tuples_ordered = [response for idx, response in extracted_tuples]\n",
    "\n",
    "    img_df[\"question_parts\"] = [ast.literal_eval(i) for i in extracted_tuples_ordered]\n",
    "    img_df[\"contains_a\"] = img_df['question_parts'].apply(lambda x: 'a' in x)\n",
    "    img_df[\"number_of_question_parts\"] = img_df['question_parts'].apply(lambda x: len(x))\n",
    "\n",
    "    # Save Extracted Numbers Table\n",
    "    extracted_question_by_page_num_df = img_df[img_df.number_of_question_parts > 0].reset_index(drop=True)\n",
    "    extracted_question_by_page_num_df = extracted_question_by_page_num_df[['page_number', \"paper_reference\", \"question_parts\"]]\n",
    "    \n",
    "    extracted_questions_savedir = Path(f\"./extracted_questions_by_page_number/{subject_id}/\")\n",
    "    extracted_questions_savedir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sanitised_paper_ref = paper_reference.replace(\"/\", \"-\")\n",
    "    extracted_question_by_page_num_df.to_csv(extracted_questions_savedir / f\"{ sanitised_paper_ref }_extracted_questions_by_page.csv\" ,index=False)\n",
    "\n",
    "else:\n",
    "    print(\"Using Cached Extracted Question Number Table\")\n",
    "    # Match DataFrames\n",
    "    extracted_question_by_page_num_df = pd.read_csv(matching_files_lst[0])\n",
    "\n",
    "    img_df = pd.merge(img_df, extracted_question_by_page_num_df[['page_number', 'question_parts']], how=\"left\", on =[\"page_number\"])\n",
    "    img_df['question_parts'] = img_df.apply(lambda x: () if pd.isnull(x['question_parts']) else ast.literal_eval(x['question_parts']), axis=1)\n",
    "    img_df[\"contains_a\"] = img_df['question_parts'].apply(lambda x: 'a' in x)\n",
    "    img_df[\"number_of_question_parts\"] = img_df['question_parts'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Question Number - presumes all pages are in correct order\n",
    "counter = 0 # starting question - 1\n",
    "q_num = []\n",
    "q_num_dict = []\n",
    "\n",
    "for i in list(img_df.contains_a):\n",
    "    counter += i\n",
    "    q_num.append( counter )\n",
    "    q_num_dict.append({\"question_number\": counter})\n",
    "\n",
    "img_df[\"question_number\"] = q_num\n",
    "\n",
    "img_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Question and Answer Text from Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Question and Answer Text from page\n",
    "def extract_question_answer_text(idx, row):\n",
    "    print(\"Processing Page Number:\\n\", row['page_number'])\n",
    "\n",
    "    if row['number_of_question_parts'] == 0:\n",
    "        prompt = extract_handwriting_prompt\n",
    "    else:\n",
    "        prompt = mcq_prompt\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"{prompt}\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{row['encoded_image_cropped']}\",\n",
    "                            \"detail\": \"auto\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1000\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    response_content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return idx, response_content\n",
    "\n",
    "extracted_questions = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    \n",
    "    # Map the row function to each row in the DataFrame, passing the index as well\n",
    "    futures = [executor.submit(extract_question_answer_text, idx, row) for idx, row in img_df.iterrows()]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            idx, response_content = future.result()\n",
    "            extracted_questions.append((idx, response_content))\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "\n",
    "# Sort the results by the original index to retain the order\n",
    "extracted_questions.sort(key=lambda x: x[0])\n",
    "\n",
    "# Extract just the responses in the correct order\n",
    "extracted_questions_ordered = [response for idx, response in extracted_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df[\"extracted_text\"] = [extract_json_from_markdown(i) for i in extracted_questions_ordered]\n",
    "\n",
    "img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Questions and Answers\n",
    "all_questions_lst = []\n",
    "for idx, q_num in enumerate(list(set(img_df.question_number))):\n",
    "\n",
    "    # Get question parts for a question number\n",
    "    l = img_df[img_df.question_number == q_num].question_parts.to_list()\n",
    "    m = [list(i) for i in l]\n",
    "\n",
    "    for j in m:\n",
    "        if len(j) == 0:\n",
    "            j.append('')\n",
    "\n",
    "    question_parts_collapsed = [item for sublist in m for item in sublist]\n",
    "    question_parts_collapsed\n",
    "\n",
    "    augmented_question_parts = []\n",
    "    for idx, question_part in enumerate(question_parts_collapsed):\n",
    "        if question_part == '':\n",
    "            augmented_question_parts.append(question_parts_collapsed[idx - 1])\n",
    "        else:\n",
    "            augmented_question_parts.append(question_part)\n",
    "\n",
    "    # Combine question number and question parts\n",
    "    extracted_questions_by_part = img_df[img_df.question_number == q_num].extracted_text.sum()\n",
    "\n",
    "    for idx, question_part in enumerate(augmented_question_parts):\n",
    "        extracted_questions_by_part[idx][\"question_part\"] = question_part\n",
    "        extracted_questions_by_part[idx][\"question_number\"] = q_num\n",
    "        if not extracted_questions_by_part[idx].get(\"question_text\"):\n",
    "            extracted_questions_by_part[idx][\"question_text\"] = \"\"\n",
    "    \n",
    "    # Align Questions and Answers\n",
    "    unique_qp_qn = list(OrderedDict.fromkeys([(i['question_number'], i[\"question_part\"]) for i in extracted_questions_by_part]))\n",
    "\n",
    "    for idx, record in enumerate(unique_qp_qn):\n",
    "        qp = record[1]\n",
    "        qn = record[0]\n",
    "\n",
    "        question_set = list(compress(extracted_questions_by_part, [(i['question_number'] == qn and i['question_part'] == qp) for i in extracted_questions_by_part]))\n",
    "        all_questions_lst.append(merge_dict_on_common_keys(question_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions_ordered_lst = [dict_reorder(i, keys_order=[\"question_number\", \"question_part\", \"question_text\", \"answer_text\"]) for i in all_questions_lst]\n",
    "all_questions_ordered_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON and Markdown\n",
    "json_object = json.dumps(all_questions_ordered_lst, indent=4)\n",
    "\n",
    "json_savedir = Path(f\"./extracted_questions_answers/{subject_id}\")\n",
    "json_savedir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(json_savedir / f\"{candidate_id}_extracted_questions.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Markdown\n",
    "markdown_doc = json_to_markdown(all_questions_ordered_lst)\n",
    "\n",
    "# Print or write the markdown document to a file\n",
    "print(markdown_doc)\n",
    "\n",
    "# write it to a file:\n",
    "with open(json_savedir / f\"{candidate_id}_extracted_questions.md\", \"w\") as f:\n",
    "  f.write(markdown_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extract_questions_from_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
